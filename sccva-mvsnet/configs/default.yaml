# === Default Config File ===
#
# Any possible parameter must be given here with description and a good default.
#


### DATA ###
DATA:
  # Name of the dataset. Some options will apply only for some datasets. Possible "replica"
  NAME: "replica"
  ROOT_DIR: "data/replica"
  # Pose extension 'gt' for ground truth poses 'dso' for DSO poses
  POSE_EXT: "gt"
  # The extension of the tuples file used. None means same as POSE_EXT. See IGNORE_POSE_SCALE for scaling.
  TUPLES_EXT: "dso_optimization_windows"
  # Whether to ignore the scale given in the tuples. Should be True for POSE_EXT==gt and False otherwise
  IGNORE_POSE_SCALE: True
  # Whether to use images rendered with mirrors
  IMAGES_WITH_MIRRORS: False
  # IMG_WIDTH must be divisible by 32
  IMG_WIDTH: 640
  IMG_HEIGHT: 480
  IMG_CHANNELS: 3
  DEPTH_MIN: 50.
  DEPTH_MAX: 250.
  DTYPE: "float32"
  # If TUPLES_DEFAULT_FLAG==True, the tuples will be made up of frames with this index distance.
  TUPLES_DEFAULT_FLAG: False
  TUPLES_DEFAULT_FRAME_DIST: 20
  TUPLES_DEFAULT_FRAME_NUM: 3
  # If normalize the depth to [0, 1]
  NORMALIZE: False


### Augmentation to use. Only used for training ###
AUG:
  # If ANY is False, no augmentation will be used at all
  # Use None for any augmentation to disable it
  ANY: False

  # Will use the same for all views of one tuple, by using kornia's same_on_batch=True
  SAME_ON_VIEWS: False

  # brightness, contrast, saturation, hue
  COLOR_JITTER: (0.05, 0.05, 0.05, 0.05)

  # kernel_size, angle (degrees), direction
  MOTION_BLUR: (11, 70., 0.5)


### MODEL ###
MODEL:
  # Number of depth planes per stage, (stage1, stage2, stage3)
  DEPTH_NUM: (48, 32, 8)
  # Ratio of this stages depth interval and the base interval. Has to start with 1.0.
  DEPTH_INTERVAL_RATIO: (1.0, 0.5, 0.25)
  # Number of channels in the cost volume per stage
  COST_VOLUME_BASE_CHANNELS: (8, 8, 8)
  # Number of channels in the 2D feature extraction net
  FEATURE_NET_BASE_CHANNELS: 8
  # Whether to use voxel wise view aggregation as in  Yi et al. Pyramid Multi-view Stereo Net with Self-adaptive View Aggregation.
  VIEW_AGGREGATION: True
  # CONV2D Normalization
  CONV2D_NORMALIZATION: batchnorm
  CONV2D_USE_BN_SKIP: False
  CONV3D_NORMALIZATION: batchnorm


### LOSS ###
LOSS:
  # Possibilities are l1_depth (recommended), depth, abs_rel, and grad
  TERMS: ("l1_depth", )
  # Recommended weights are: l1_depth: 1.0, abs_rel: 1.0, depth: 1.0, grad: 0.01
  TERM_WEIGHTS: (1.0, )
  # Weights for the loss at each stage.
  STAGE_WEIGHTS: (1.0, 1.0, 1.0)


### TRAIN ###
TRAIN:
  EPOCHS: 50
  BATCH_SIZE: 1
  LR: 0.0001
  # In the last epoch, the learning rate will be LR*LR_SCHEDULE_FINAL_FRACTION
  LR_SCHEDULE_FINAL_FRACTION: 0.01
  # When using ddp the effective batch size is batch_size * num_nodes * num_gpus_per_node
  # to achieve similar training to the serial case, the learning rate should be scaled
  # accordingly: lr_ddp = batch_size_ddp/batch_size * lr = num_nodes * num_gpus_per_node * lr
  LR_DDP_SCALE_WITH_BATCH_SIZE: True
  # Options cpu, cuda (for one GPU), slurm-ddp (has to  use sbatch for launching), debug-ddp-num_nodes-num_gpus_per_node
  # For debug-ddp-1-1 one has to launch with MASTER_ADDR=localhost MASTER_PORT=12345
  # WORLD_SIZE=1 NODE_RANK=0 LOCAL_RANK=0 python train.py ...
  # See: https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html#distributed-data-parallel
  DEVICE: "cuda"
  NUM_WORKERS: 6
  SHUFFLE: True
  DROP_LAST: True
  SEED: 1234
  # torch.backends.cudnn.benchmark set to True for good performance.
  # Might have to set false on 48G machine: https://github.com/pytorch/pytorch/issues/45769
  CUDNN_BENCHMARK: True
  USE_SPARSE: False
  SC2DC: False
  NS: False


### IO ###
IO:
  LEVEL: "INFO"
  LOG_INTERVAL: 50
  # Weights summary at beginning of training ("full", "top", None)
  WEIGHTS_SUMMARY: None
  # Possible summaries to show in tensorboard: possibilities ("image", "depth", "confidence")
  SUMMARIES: ("image", "depth", "confidence")
  # The number of samples per step used for scaling the x-axis of tensorboard correctly. This is set inside train.py.
  SAMPLES_PER_STEP: -1
